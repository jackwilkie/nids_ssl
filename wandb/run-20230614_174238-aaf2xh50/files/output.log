epoch = 0  |  train_loss = 0.101922  |  val_loss = 0.092211  |  training_for: 1.84
epoch = 1  |  train_loss = 0.094605  |  val_loss = 0.089343  |  training_for: 3.29
epoch = 2  |  train_loss = 0.092429  |  val_loss = 0.090601  |  training_for: 4.74
epoch = 3  |  train_loss = 0.091737  |  val_loss = 0.088582  |  training_for: 6.15
epoch = 4  |  train_loss = 0.091292  |  val_loss = 0.089734  |  training_for: 7.57
epoch = 5  |  train_loss = 0.091078  |  val_loss = 0.087910  |  training_for: 9.09
epoch = 6  |  train_loss = 0.090938  |  val_loss = 0.088315  |  training_for: 10.51
epoch = 7  |  train_loss = 0.090868  |  val_loss = 0.089157  |  training_for: 11.94
epoch = 8  |  train_loss = 0.090746  |  val_loss = 0.088751  |  training_for: 13.36
epoch = 9  |  train_loss = 0.090658  |  val_loss = 0.089066  |  training_for: 14.77
epoch = 10  |  train_loss = 0.090640  |  val_loss = 0.089228  |  training_for: 16.19
epoch = 11  |  train_loss = 0.090629  |  val_loss = 0.089211  |  training_for: 17.62
epoch = 12  |  train_loss = 0.090592  |  val_loss = 0.090165  |  training_for: 19.04
epoch = 13  |  train_loss = 0.090605  |  val_loss = 0.089603  |  training_for: 20.46
epoch = 14  |  train_loss = 0.090540  |  val_loss = 0.089623  |  training_for: 21.89
epoch = 15  |  train_loss = 0.090556  |  val_loss = 0.089092  |  training_for: 23.42
epoch = 16  |  train_loss = 0.090574  |  val_loss = 0.089412  |  training_for: 24.85
epoch = 17  |  train_loss = 0.090519  |  val_loss = 0.090050  |  training_for: 26.28
epoch = 18  |  train_loss = 0.090511  |  val_loss = 0.090592  |  training_for: 27.70
epoch = 19  |  train_loss = 0.090505  |  val_loss = 0.090732  |  training_for: 29.14
epoch = 20  |  train_loss = 0.090504  |  val_loss = 0.090501  |  training_for: 30.56
epoch = 21  |  train_loss = 0.090457  |  val_loss = 0.090487  |  training_for: 32.00
epoch = 22  |  train_loss = 0.090401  |  val_loss = 0.090181  |  training_for: 33.51
epoch = 23  |  train_loss = 0.090351  |  val_loss = 0.089546  |  training_for: 34.94
epoch = 24  |  train_loss = 0.090328  |  val_loss = 0.089344  |  training_for: 36.36
epoch = 25  |  train_loss = 0.090346  |  val_loss = 0.090593  |  training_for: 37.84
epoch = 26  |  train_loss = 0.090339  |  val_loss = 0.089935  |  training_for: 39.26
epoch = 27  |  train_loss = 0.090383  |  val_loss = 0.089219  |  training_for: 40.67
epoch = 28  |  train_loss = 0.090340  |  val_loss = 0.090849  |  training_for: 42.10
epoch = 29  |  train_loss = 0.090349  |  val_loss = 0.090130  |  training_for: 43.53
epoch = 30  |  train_loss = 0.090364  |  val_loss = 0.090258  |  training_for: 44.97
epoch = 31  |  train_loss = 0.090304  |  val_loss = 0.090156  |  training_for: 46.39
epoch = 32  |  train_loss = 0.090296  |  val_loss = 0.089438  |  training_for: 47.91
epoch = 33  |  train_loss = 0.090319  |  val_loss = 0.089745  |  training_for: 49.35
epoch = 34  |  train_loss = 0.090342  |  val_loss = 0.090257  |  training_for: 50.77
epoch = 35  |  train_loss = 0.090296  |  val_loss = 0.090181  |  training_for: 52.19
epoch = 36  |  train_loss = 0.090352  |  val_loss = 0.091484  |  training_for: 53.62
epoch = 37  |  train_loss = 0.090336  |  val_loss = 0.091589  |  training_for: 55.07
epoch = 38  |  train_loss = 0.090289  |  val_loss = 0.090989  |  training_for: 56.52
epoch = 39  |  train_loss = 0.090290  |  val_loss = 0.090588  |  training_for: 58.06
epoch = 40  |  train_loss = 0.090276  |  val_loss = 0.091071  |  training_for: 59.48
epoch = 41  |  train_loss = 0.090264  |  val_loss = 0.090253  |  training_for: 60.90
epoch = 42  |  train_loss = 0.090222  |  val_loss = 0.089874  |  training_for: 62.34
epoch = 43  |  train_loss = 0.090221  |  val_loss = 0.090403  |  training_for: 63.77
epoch = 44  |  train_loss = 0.090205  |  val_loss = 0.090911  |  training_for: 65.20
epoch = 45  |  train_loss = 0.090210  |  val_loss = 0.090576  |  training_for: 66.62
epoch = 46  |  train_loss = 0.090231  |  val_loss = 0.090512  |  training_for: 68.04
epoch = 47  |  train_loss = 0.090220  |  val_loss = 0.091007  |  training_for: 69.47
epoch = 48  |  train_loss = 0.090232  |  val_loss = 0.090996  |  training_for: 70.91
epoch = 49  |  train_loss = 0.090233  |  val_loss = 0.090715  |  training_for: 72.33
epoch = 50  |  train_loss = 0.090238  |  val_loss = 0.090192  |  training_for: 73.86
epoch = 51  |  train_loss = 0.090272  |  val_loss = 0.090864  |  training_for: 75.28
epoch = 52  |  train_loss = 0.090282  |  val_loss = 0.091622  |  training_for: 76.70
epoch = 53  |  train_loss = 0.090234  |  val_loss = 0.091234  |  training_for: 78.11
epoch = 54  |  train_loss = 0.090243  |  val_loss = 0.090763  |  training_for: 79.56
epoch = 55  |  train_loss = 0.090216  |  val_loss = 0.091482  |  training_for: 80.99
epoch = 56  |  train_loss = 0.090233  |  val_loss = 0.091555  |  training_for: 82.50
epoch = 57  |  train_loss = 0.090274  |  val_loss = 0.090552  |  training_for: 83.92
epoch = 58  |  train_loss = 0.090277  |  val_loss = 0.090177  |  training_for: 85.34
epoch = 59  |  train_loss = 0.090263  |  val_loss = 0.091027  |  training_for: 86.77
epoch = 60  |  train_loss = 0.090206  |  val_loss = 0.090929  |  training_for: 88.20
epoch = 61  |  train_loss = 0.090206  |  val_loss = 0.091286  |  training_for: 89.66
epoch = 62  |  train_loss = 0.090215  |  val_loss = 0.090739  |  training_for: 91.09
epoch = 63  |  train_loss = 0.090210  |  val_loss = 0.090982  |  training_for: 92.51
Traceback (most recent call last):
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 131, in <module>
    main()
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 117, in main
    train(model = model,
  File "/home/jwilkie/code_base/packages/model_training/training_loops.py", line 152, in train
    val_loss += loss.item() * batch[0].size(0)  # multiply sample loss by batch size for batch loss
KeyboardInterrupt