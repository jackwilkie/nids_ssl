epoch: 0
Memory Allocated: 19602944
Max Memory Allocated: 19602944
epoch = 0  |  train_loss = 0.179675  |  val_loss = 0.175368  |  training_for: 1.79
epoch: 1
Memory Allocated: 77421056
Max Memory Allocated: 319610880
epoch = 1  |  train_loss = 0.177653  |  val_loss = 0.174439  |  training_for: 3.19
epoch: 2
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 2  |  train_loss = 0.174837  |  val_loss = 0.172771  |  training_for: 4.62
epoch: 3
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 3  |  train_loss = 0.171281  |  val_loss = 0.170749  |  training_for: 6.02
epoch: 4
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 4  |  train_loss = 0.167678  |  val_loss = 0.169447  |  training_for: 7.48
epoch: 5
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 5  |  train_loss = 0.164835  |  val_loss = 0.167906  |  training_for: 8.88
epoch: 6
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 6  |  train_loss = 0.162467  |  val_loss = 0.165322  |  training_for: 10.28
epoch: 7
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 7  |  train_loss = 0.160312  |  val_loss = 0.162704  |  training_for: 11.72
epoch: 8
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 8  |  train_loss = 0.158516  |  val_loss = 0.160928  |  training_for: 13.17
epoch: 9
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 9  |  train_loss = 0.157090  |  val_loss = 0.160840  |  training_for: 14.58
epoch: 10
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 10  |  train_loss = 0.156037  |  val_loss = 0.161769  |  training_for: 16.07
epoch: 11
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 11  |  train_loss = 0.155255  |  val_loss = 0.162759  |  training_for: 17.49
epoch: 12
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 12  |  train_loss = 0.154631  |  val_loss = 0.163013  |  training_for: 18.89
epoch: 13
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 13  |  train_loss = 0.154127  |  val_loss = 0.162851  |  training_for: 20.29
epoch: 14
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 14  |  train_loss = 0.153687  |  val_loss = 0.162668  |  training_for: 21.71
epoch: 15
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 15  |  train_loss = 0.153318  |  val_loss = 0.162256  |  training_for: 23.10
epoch: 16
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 16  |  train_loss = 0.153030  |  val_loss = 0.161934  |  training_for: 24.51
epoch: 17
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 17  |  train_loss = 0.152763  |  val_loss = 0.161709  |  training_for: 25.94
epoch: 18
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 18  |  train_loss = 0.152514  |  val_loss = 0.161254  |  training_for: 27.36
epoch: 19
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 19  |  train_loss = 0.152290  |  val_loss = 0.160794  |  training_for: 28.77
epoch: 20
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 20  |  train_loss = 0.152096  |  val_loss = 0.160087  |  training_for: 30.16
epoch: 21
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 21  |  train_loss = 0.151943  |  val_loss = 0.159686  |  training_for: 31.59
epoch: 22
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 22  |  train_loss = 0.151836  |  val_loss = 0.160091  |  training_for: 33.04
epoch: 23
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 23  |  train_loss = 0.151698  |  val_loss = 0.160016  |  training_for: 34.55
epoch: 24
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 24  |  train_loss = 0.151560  |  val_loss = 0.160191  |  training_for: 35.98
epoch: 25
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 25  |  train_loss = 0.151435  |  val_loss = 0.160090  |  training_for: 37.38
epoch: 26
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 26  |  train_loss = 0.151343  |  val_loss = 0.160071  |  training_for: 38.76
epoch: 27
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 27  |  train_loss = 0.151263  |  val_loss = 0.160083  |  training_for: 40.21
epoch: 28
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 28  |  train_loss = 0.151197  |  val_loss = 0.160409  |  training_for: 41.61
epoch: 29
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 29  |  train_loss = 0.151105  |  val_loss = 0.160095  |  training_for: 43.01
epoch: 30
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 30  |  train_loss = 0.151071  |  val_loss = 0.160100  |  training_for: 44.42
epoch: 31
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 31  |  train_loss = 0.150972  |  val_loss = 0.160315  |  training_for: 45.80
epoch: 32
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 32  |  train_loss = 0.150939  |  val_loss = 0.160376  |  training_for: 47.22
epoch: 33
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 33  |  train_loss = 0.150882  |  val_loss = 0.160236  |  training_for: 48.63
epoch: 34
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 34  |  train_loss = 0.150828  |  val_loss = 0.160437  |  training_for: 50.02
epoch: 35
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 35  |  train_loss = 0.150779  |  val_loss = 0.160395  |  training_for: 51.45
epoch: 36
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 36  |  train_loss = 0.150715  |  val_loss = 0.160170  |  training_for: 52.86
epoch: 37
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 37  |  train_loss = 0.150696  |  val_loss = 0.161011  |  training_for: 54.36
epoch: 38
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 38  |  train_loss = 0.150633  |  val_loss = 0.160589  |  training_for: 55.77
epoch: 39
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 39  |  train_loss = 0.150577  |  val_loss = 0.160837  |  training_for: 57.18
epoch: 40
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 40  |  train_loss = 0.150573  |  val_loss = 0.160653  |  training_for: 58.58
epoch: 41
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 41  |  train_loss = 0.150527  |  val_loss = 0.160995  |  training_for: 60.01
epoch: 42
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 42  |  train_loss = 0.150480  |  val_loss = 0.161271  |  training_for: 61.44
epoch: 43
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 43  |  train_loss = 0.150421  |  val_loss = 0.161254  |  training_for: 62.84
epoch: 44
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 44  |  train_loss = 0.150394  |  val_loss = 0.161394  |  training_for: 64.25
epoch: 45
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 45  |  train_loss = 0.150370  |  val_loss = 0.161549  |  training_for: 65.70
epoch: 46
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 46  |  train_loss = 0.150369  |  val_loss = 0.161985  |  training_for: 67.09
epoch: 47
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 47  |  train_loss = 0.150329  |  val_loss = 0.161504  |  training_for: 68.48
epoch: 48
Memory Allocated: 75696640
Max Memory Allocated: 359347712
epoch = 48  |  train_loss = 0.150278  |  val_loss = 0.162493  |  training_for: 69.90
epoch: 49
Memory Allocated: 75696640
Max Memory Allocated: 359347712
Traceback (most recent call last):
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 130, in <module>
    main()
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 116, in main
    train(model = model,
  File "/home/jwilkie/code_base/packages/model_training/training_loops.py", line 149, in train
    loss = loss_calc(model, batch)  #compute validation loss
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/composite_loss.py", line 46, in calc_loss
    z_aug = model.feed_aug(x)
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/saint.py", line 141, in feed_aug
    return self.encoder(self.latent_aug(self.embedding_layer(self.feature_aug(x))))
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/encoders.py", line 57, in forward
    if self.norm_output: x = self.norm(x) # normalise output if required
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
KeyboardInterrupt