
epoch = 0  |  train_loss = 0.107120  |  val_loss = 0.096198  |  training_for: 3.82
epoch = 1  |  train_loss = 0.107077  |  val_loss = 0.096162  |  training_for: 7.30
epoch = 2  |  train_loss = 0.107003  |  val_loss = 0.096093  |  training_for: 10.77
epoch = 3  |  train_loss = 0.106867  |  val_loss = 0.095992  |  training_for: 14.23
epoch = 4  |  train_loss = 0.106685  |  val_loss = 0.095858  |  training_for: 17.69
epoch = 5  |  train_loss = 0.106464  |  val_loss = 0.095682  |  training_for: 21.24
epoch = 6  |  train_loss = 0.106152  |  val_loss = 0.095460  |  training_for: 24.69
epoch = 7  |  train_loss = 0.105761  |  val_loss = 0.095166  |  training_for: 28.15
epoch = 8  |  train_loss = 0.105280  |  val_loss = 0.094792  |  training_for: 31.60
epoch = 9  |  train_loss = 0.104671  |  val_loss = 0.094323  |  training_for: 35.06
epoch = 10  |  train_loss = 0.103942  |  val_loss = 0.093738  |  training_for: 38.51
epoch = 11  |  train_loss = 0.103056  |  val_loss = 0.093054  |  training_for: 41.96
epoch = 12  |  train_loss = 0.102066  |  val_loss = 0.092232  |  training_for: 45.42
epoch = 13  |  train_loss = 0.100932  |  val_loss = 0.091320  |  training_for: 48.87
epoch = 14  |  train_loss = 0.099711  |  val_loss = 0.090355  |  training_for: 52.42
epoch = 15  |  train_loss = 0.098498  |  val_loss = 0.089441  |  training_for: 55.88
epoch = 16  |  train_loss = 0.097333  |  val_loss = 0.088663  |  training_for: 59.34
epoch = 17  |  train_loss = 0.096337  |  val_loss = 0.088048  |  training_for: 62.80
epoch = 18  |  train_loss = 0.095506  |  val_loss = 0.087553  |  training_for: 66.26
epoch = 19  |  train_loss = 0.094801  |  val_loss = 0.087170  |  training_for: 69.71
epoch = 20  |  train_loss = 0.094178  |  val_loss = 0.086851  |  training_for: 73.16
epoch = 21  |  train_loss = 0.093634  |  val_loss = 0.086558  |  training_for: 76.71
epoch = 22  |  train_loss = 0.093120  |  val_loss = 0.086273  |  training_for: 80.17
epoch = 23  |  train_loss = 0.092679  |  val_loss = 0.086043  |  training_for: 83.63
epoch = 24  |  train_loss = 0.092297  |  val_loss = 0.085889  |  training_for: 87.08
epoch = 25  |  train_loss = 0.091973  |  val_loss = 0.085861  |  training_for: 90.54
epoch = 26  |  train_loss = 0.091695  |  val_loss = 0.085864  |  training_for: 94.00
epoch = 27  |  train_loss = 0.091472  |  val_loss = 0.085888  |  training_for: 97.45
epoch = 28  |  train_loss = 0.091255  |  val_loss = 0.085739  |  training_for: 100.90
epoch = 29  |  train_loss = 0.091089  |  val_loss = 0.085728  |  training_for: 104.36
epoch = 30  |  train_loss = 0.090926  |  val_loss = 0.085766  |  training_for: 107.82
epoch = 31  |  train_loss = 0.090809  |  val_loss = 0.085723  |  training_for: 111.36
epoch = 32  |  train_loss = 0.090712  |  val_loss = 0.085764  |  training_for: 114.81
epoch = 33  |  train_loss = 0.090622  |  val_loss = 0.085884  |  training_for: 118.27
epoch = 34  |  train_loss = 0.090558  |  val_loss = 0.085766  |  training_for: 121.72
epoch = 35  |  train_loss = 0.090495  |  val_loss = 0.085902  |  training_for: 125.18
epoch = 36  |  train_loss = 0.090445  |  val_loss = 0.085874  |  training_for: 128.63
epoch = 37  |  train_loss = 0.090399  |  val_loss = 0.086072  |  training_for: 132.19
epoch = 38  |  train_loss = 0.090362  |  val_loss = 0.086181  |  training_for: 135.66
epoch = 39  |  train_loss = 0.090334  |  val_loss = 0.086047  |  training_for: 139.11
epoch = 40  |  train_loss = 0.090303  |  val_loss = 0.086357  |  training_for: 142.56
epoch = 41  |  train_loss = 0.090278  |  val_loss = 0.086353  |  training_for: 146.04
epoch = 42  |  train_loss = 0.090252  |  val_loss = 0.086265  |  training_for: 149.49
epoch = 43  |  train_loss = 0.090232  |  val_loss = 0.086542  |  training_for: 152.94
epoch = 44  |  train_loss = 0.090209  |  val_loss = 0.086320  |  training_for: 156.39
epoch = 45  |  train_loss = 0.090191  |  val_loss = 0.086564  |  training_for: 159.84
epoch = 46  |  train_loss = 0.090176  |  val_loss = 0.086367  |  training_for: 163.31
epoch = 47  |  train_loss = 0.090156  |  val_loss = 0.086662  |  training_for: 166.86
epoch = 48  |  train_loss = 0.090151  |  val_loss = 0.086554  |  training_for: 170.31
epoch = 49  |  train_loss = 0.090142  |  val_loss = 0.086860  |  training_for: 173.79
epoch = 50  |  train_loss = 0.090119  |  val_loss = 0.086459  |  training_for: 177.24
epoch = 51  |  train_loss = 0.090102  |  val_loss = 0.086768  |  training_for: 180.69
epoch = 52  |  train_loss = 0.090088  |  val_loss = 0.086748  |  training_for: 184.15
epoch = 53  |  train_loss = 0.090074  |  val_loss = 0.086566  |  training_for: 187.70
epoch = 54  |  train_loss = 0.090064  |  val_loss = 0.086816  |  training_for: 191.15
epoch = 55  |  train_loss = 0.090053  |  val_loss = 0.086832  |  training_for: 194.59
epoch = 56  |  train_loss = 0.090049  |  val_loss = 0.086646  |  training_for: 198.04
Traceback (most recent call last):
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 132, in <module>
    main()
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 118, in main
    train(model = model,
  File "/home/jwilkie/code_base/packages/model_training/training_loops.py", line 150, in train
    loss = loss_calc(model, batch)  #compute validation loss
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/composite_loss.py", line 46, in calc_loss
    z_aug = model.feed_aug(x)
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/saint.py", line 141, in feed_aug
    return self.encoder(self.latent_aug(self.embedding_layer(self.feature_aug(x))))
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/augmentations.py", line 54, in forward
    return self.augment(x)
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/augmentations.py", line 109, in augment
    x_shuffled = shuffle_batch(x)  # batch x d_model, generate batch with samples shuffled ( shuffles dim 0)
KeyboardInterrupt