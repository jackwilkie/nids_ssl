epoch = 0  |  train_loss = 0.107490  |  val_loss = 0.095623  |  training_for: 1.35
epoch = 1  |  train_loss = 0.105983  |  val_loss = 0.094432  |  training_for: 2.33
epoch = 2  |  train_loss = 0.103911  |  val_loss = 0.092843  |  training_for: 3.30
epoch = 3  |  train_loss = 0.101399  |  val_loss = 0.090975  |  training_for: 4.28
epoch = 4  |  train_loss = 0.098718  |  val_loss = 0.089281  |  training_for: 5.29
epoch = 5  |  train_loss = 0.096487  |  val_loss = 0.088665  |  training_for: 6.27
epoch = 6  |  train_loss = 0.094951  |  val_loss = 0.088274  |  training_for: 7.26
epoch = 7  |  train_loss = 0.093928  |  val_loss = 0.088304  |  training_for: 8.27
epoch = 8  |  train_loss = 0.093339  |  val_loss = 0.088513  |  training_for: 9.35
epoch = 9  |  train_loss = 0.092948  |  val_loss = 0.088567  |  training_for: 10.33
epoch = 10  |  train_loss = 0.092662  |  val_loss = 0.088587  |  training_for: 11.31
epoch = 11  |  train_loss = 0.092477  |  val_loss = 0.088631  |  training_for: 12.30
epoch = 12  |  train_loss = 0.092297  |  val_loss = 0.088788  |  training_for: 13.29
epoch = 13  |  train_loss = 0.092151  |  val_loss = 0.089015  |  training_for: 14.28
epoch = 14  |  train_loss = 0.092013  |  val_loss = 0.089120  |  training_for: 15.30
epoch = 15  |  train_loss = 0.091897  |  val_loss = 0.089329  |  training_for: 16.42
epoch = 16  |  train_loss = 0.091795  |  val_loss = 0.089304  |  training_for: 17.39
epoch = 17  |  train_loss = 0.091705  |  val_loss = 0.089380  |  training_for: 18.39
epoch = 18  |  train_loss = 0.091629  |  val_loss = 0.089586  |  training_for: 19.42
epoch = 19  |  train_loss = 0.091562  |  val_loss = 0.089631  |  training_for: 20.40
epoch = 20  |  train_loss = 0.091497  |  val_loss = 0.089882  |  training_for: 21.42
epoch = 21  |  train_loss = 0.091439  |  val_loss = 0.089854  |  training_for: 22.40
epoch = 22  |  train_loss = 0.091378  |  val_loss = 0.089942  |  training_for: 23.41
epoch = 23  |  train_loss = 0.091337  |  val_loss = 0.089889  |  training_for: 24.40
epoch = 24  |  train_loss = 0.091296  |  val_loss = 0.089984  |  training_for: 25.38
epoch = 25  |  train_loss = 0.091244  |  val_loss = 0.089943  |  training_for: 26.48
epoch = 26  |  train_loss = 0.091215  |  val_loss = 0.090254  |  training_for: 27.46
epoch = 27  |  train_loss = 0.091175  |  val_loss = 0.089982  |  training_for: 28.46
epoch = 28  |  train_loss = 0.091130  |  val_loss = 0.089908  |  training_for: 29.44
epoch = 29  |  train_loss = 0.091085  |  val_loss = 0.089832  |  training_for: 30.44
epoch = 30  |  train_loss = 0.091025  |  val_loss = 0.089745  |  training_for: 31.41
epoch = 31  |  train_loss = 0.090989  |  val_loss = 0.089802  |  training_for: 32.38
epoch = 32  |  train_loss = 0.090951  |  val_loss = 0.089749  |  training_for: 33.37
epoch = 33  |  train_loss = 0.090919  |  val_loss = 0.089517  |  training_for: 34.33
epoch = 34  |  train_loss = 0.090885  |  val_loss = 0.089579  |  training_for: 35.30
epoch = 35  |  train_loss = 0.090860  |  val_loss = 0.089605  |  training_for: 36.29
epoch = 36  |  train_loss = 0.090838  |  val_loss = 0.089465  |  training_for: 37.36
epoch = 37  |  train_loss = 0.090815  |  val_loss = 0.089515  |  training_for: 38.36
epoch = 38  |  train_loss = 0.090791  |  val_loss = 0.089453  |  training_for: 39.35
epoch = 39  |  train_loss = 0.090767  |  val_loss = 0.089566  |  training_for: 40.35
epoch = 40  |  train_loss = 0.090756  |  val_loss = 0.089337  |  training_for: 41.32
epoch = 41  |  train_loss = 0.090734  |  val_loss = 0.089425  |  training_for: 42.30
epoch = 42  |  train_loss = 0.090724  |  val_loss = 0.089487  |  training_for: 43.28
epoch = 43  |  train_loss = 0.090703  |  val_loss = 0.089736  |  training_for: 44.38
epoch = 44  |  train_loss = 0.090696  |  val_loss = 0.089513  |  training_for: 45.37
epoch = 45  |  train_loss = 0.090697  |  val_loss = 0.089467  |  training_for: 46.36
epoch = 46  |  train_loss = 0.090672  |  val_loss = 0.089568  |  training_for: 47.35
epoch = 47  |  train_loss = 0.090659  |  val_loss = 0.089719  |  training_for: 48.33
epoch = 48  |  train_loss = 0.090653  |  val_loss = 0.089620  |  training_for: 49.31
epoch = 49  |  train_loss = 0.090645  |  val_loss = 0.089551  |  training_for: 50.29
epoch = 50  |  train_loss = 0.090631  |  val_loss = 0.089782  |  training_for: 51.27
epoch = 51  |  train_loss = 0.090621  |  val_loss = 0.089779  |  training_for: 52.26
epoch = 52  |  train_loss = 0.090616  |  val_loss = 0.089891  |  training_for: 53.24
epoch = 53  |  train_loss = 0.090604  |  val_loss = 0.089808  |  training_for: 54.23
epoch = 54  |  train_loss = 0.090599  |  val_loss = 0.089868  |  training_for: 55.31
epoch = 55  |  train_loss = 0.090591  |  val_loss = 0.089992  |  training_for: 56.30
epoch = 56  |  train_loss = 0.090581  |  val_loss = 0.089823  |  training_for: 57.29
epoch = 57  |  train_loss = 0.090577  |  val_loss = 0.089972  |  training_for: 58.27
epoch = 58  |  train_loss = 0.090577  |  val_loss = 0.089733  |  training_for: 59.26
epoch = 59  |  train_loss = 0.090567  |  val_loss = 0.089952  |  training_for: 60.24
epoch = 60  |  train_loss = 0.090562  |  val_loss = 0.089894  |  training_for: 61.22
epoch = 61  |  train_loss = 0.090557  |  val_loss = 0.090222  |  training_for: 62.29
epoch = 62  |  train_loss = 0.090555  |  val_loss = 0.089921  |  training_for: 63.26
epoch = 63  |  train_loss = 0.090547  |  val_loss = 0.090229  |  training_for: 64.26
epoch = 64  |  train_loss = 0.090537  |  val_loss = 0.089872  |  training_for: 65.23
epoch = 65  |  train_loss = 0.090539  |  val_loss = 0.090238  |  training_for: 66.21
epoch = 66  |  train_loss = 0.090533  |  val_loss = 0.090048  |  training_for: 67.19
epoch = 67  |  train_loss = 0.090523  |  val_loss = 0.090237  |  training_for: 68.19
epoch = 68  |  train_loss = 0.090524  |  val_loss = 0.089920  |  training_for: 69.22
epoch = 69  |  train_loss = 0.090517  |  val_loss = 0.090207  |  training_for: 70.21
epoch = 70  |  train_loss = 0.090517  |  val_loss = 0.090002  |  training_for: 71.18
epoch = 71  |  train_loss = 0.090514  |  val_loss = 0.090480  |  training_for: 72.19
epoch = 72  |  train_loss = 0.090512  |  val_loss = 0.090041  |  training_for: 73.27
epoch = 73  |  train_loss = 0.090505  |  val_loss = 0.090043  |  training_for: 74.26
epoch = 74  |  train_loss = 0.090498  |  val_loss = 0.090168  |  training_for: 75.27
epoch = 75  |  train_loss = 0.090496  |  val_loss = 0.090274  |  training_for: 76.25
epoch = 76  |  train_loss = 0.090485  |  val_loss = 0.090179  |  training_for: 77.22
epoch = 77  |  train_loss = 0.090486  |  val_loss = 0.090263  |  training_for: 78.24
epoch = 78  |  train_loss = 0.090499  |  val_loss = 0.090020  |  training_for: 79.23
epoch = 79  |  train_loss = 0.090485  |  val_loss = 0.090123  |  training_for: 80.32
epoch = 80  |  train_loss = 0.090477  |  val_loss = 0.090236  |  training_for: 81.30
epoch = 81  |  train_loss = 0.090483  |  val_loss = 0.090190  |  training_for: 82.29
epoch = 82  |  train_loss = 0.090478  |  val_loss = 0.090269  |  training_for: 83.30
epoch = 83  |  train_loss = 0.090475  |  val_loss = 0.090088  |  training_for: 84.29
epoch = 84  |  train_loss = 0.090478  |  val_loss = 0.090181  |  training_for: 85.27
epoch = 85  |  train_loss = 0.090480  |  val_loss = 0.090266  |  training_for: 86.25
epoch = 86  |  train_loss = 0.090466  |  val_loss = 0.090238  |  training_for: 87.25
epoch = 87  |  train_loss = 0.090466  |  val_loss = 0.089887  |  training_for: 88.23
epoch = 88  |  train_loss = 0.090465  |  val_loss = 0.090268  |  training_for: 89.21
epoch = 89  |  train_loss = 0.090458  |  val_loss = 0.090061  |  training_for: 90.20
epoch = 90  |  train_loss = 0.090457  |  val_loss = 0.090168  |  training_for: 91.28
epoch = 91  |  train_loss = 0.090457  |  val_loss = 0.089998  |  training_for: 92.31
epoch = 92  |  train_loss = 0.090446  |  val_loss = 0.090124  |  training_for: 93.29
epoch = 93  |  train_loss = 0.090450  |  val_loss = 0.090119  |  training_for: 94.27
epoch = 94  |  train_loss = 0.090455  |  val_loss = 0.090262  |  training_for: 95.23
epoch = 95  |  train_loss = 0.090444  |  val_loss = 0.090005  |  training_for: 96.21
epoch = 96  |  train_loss = 0.090443  |  val_loss = 0.090262  |  training_for: 97.19
epoch = 97  |  train_loss = 0.090442  |  val_loss = 0.090133  |  training_for: 98.26
epoch = 98  |  train_loss = 0.090435  |  val_loss = 0.090031  |  training_for: 99.24
epoch = 99  |  train_loss = 0.090437  |  val_loss = 0.090122  |  training_for: 100.23
epoch = 100  |  train_loss = 0.090434  |  val_loss = 0.090180  |  training_for: 101.20
epoch = 101  |  train_loss = 0.090428  |  val_loss = 0.089867  |  training_for: 102.18
epoch = 102  |  train_loss = 0.090428  |  val_loss = 0.090182  |  training_for: 103.15
epoch = 103  |  train_loss = 0.090428  |  val_loss = 0.090112  |  training_for: 104.14
epoch = 104  |  train_loss = 0.090424  |  val_loss = 0.089966  |  training_for: 105.13
epoch = 105  |  train_loss = 0.090418  |  val_loss = 0.090160  |  training_for: 106.11
epoch = 106  |  train_loss = 0.090411  |  val_loss = 0.090007  |  training_for: 107.09
epoch = 107  |  train_loss = 0.090413  |  val_loss = 0.089918  |  training_for: 108.07
epoch = 108  |  train_loss = 0.090411  |  val_loss = 0.090239  |  training_for: 109.15
epoch = 109  |  train_loss = 0.090411  |  val_loss = 0.090203  |  training_for: 110.13
epoch = 110  |  train_loss = 0.090421  |  val_loss = 0.090054  |  training_for: 111.11
epoch = 111  |  train_loss = 0.090412  |  val_loss = 0.090063  |  training_for: 112.09
epoch = 112  |  train_loss = 0.090405  |  val_loss = 0.090292  |  training_for: 113.07
epoch = 113  |  train_loss = 0.090414  |  val_loss = 0.089969  |  training_for: 114.10
epoch = 114  |  train_loss = 0.090414  |  val_loss = 0.089863  |  training_for: 115.18
epoch = 115  |  train_loss = 0.090407  |  val_loss = 0.090278  |  training_for: 116.18
epoch = 116  |  train_loss = 0.090403  |  val_loss = 0.089900  |  training_for: 117.19
epoch = 117  |  train_loss = 0.090409  |  val_loss = 0.090372  |  training_for: 118.16
epoch = 118  |  train_loss = 0.090407  |  val_loss = 0.090137  |  training_for: 119.13
epoch = 119  |  train_loss = 0.090406  |  val_loss = 0.090204  |  training_for: 120.12
epoch = 120  |  train_loss = 0.090400  |  val_loss = 0.090252  |  training_for: 121.09
epoch = 121  |  train_loss = 0.090392  |  val_loss = 0.090054  |  training_for: 122.08
epoch = 122  |  train_loss = 0.090389  |  val_loss = 0.089990  |  training_for: 123.06
epoch = 123  |  train_loss = 0.090397  |  val_loss = 0.090486  |  training_for: 124.04
epoch = 124  |  train_loss = 0.090389  |  val_loss = 0.089813  |  training_for: 125.02
epoch = 125  |  train_loss = 0.090388  |  val_loss = 0.090287  |  training_for: 126.00
epoch = 126  |  train_loss = 0.090392  |  val_loss = 0.090193  |  training_for: 127.08
epoch = 127  |  train_loss = 0.090381  |  val_loss = 0.090137  |  training_for: 128.06
epoch = 128  |  train_loss = 0.090377  |  val_loss = 0.090169  |  training_for: 129.05
epoch = 129  |  train_loss = 0.090380  |  val_loss = 0.089984  |  training_for: 130.04
epoch = 130  |  train_loss = 0.090377  |  val_loss = 0.090183  |  training_for: 131.03
epoch = 131  |  train_loss = 0.090383  |  val_loss = 0.090191  |  training_for: 132.02
epoch = 132  |  train_loss = 0.090378  |  val_loss = 0.089976  |  training_for: 133.10
epoch = 133  |  train_loss = 0.090379  |  val_loss = 0.090236  |  training_for: 134.08
epoch = 134  |  train_loss = 0.090373  |  val_loss = 0.090094  |  training_for: 135.09
epoch = 135  |  train_loss = 0.090380  |  val_loss = 0.090321  |  training_for: 136.07
epoch = 136  |  train_loss = 0.090382  |  val_loss = 0.090109  |  training_for: 137.05
epoch = 137  |  train_loss = 0.090379  |  val_loss = 0.090223  |  training_for: 138.03
epoch = 138  |  train_loss = 0.090381  |  val_loss = 0.090111  |  training_for: 139.00
epoch = 139  |  train_loss = 0.090375  |  val_loss = 0.090302  |  training_for: 139.99
epoch = 140  |  train_loss = 0.090371  |  val_loss = 0.090032  |  training_for: 140.99
epoch = 141  |  train_loss = 0.090376  |  val_loss = 0.090111  |  training_for: 141.96
epoch = 142  |  train_loss = 0.090369  |  val_loss = 0.090079  |  training_for: 142.93
epoch = 143  |  train_loss = 0.090369  |  val_loss = 0.090220  |  training_for: 144.02
epoch = 144  |  train_loss = 0.090366  |  val_loss = 0.089915  |  training_for: 145.00
epoch = 145  |  train_loss = 0.090364  |  val_loss = 0.090277  |  training_for: 145.98
epoch = 146  |  train_loss = 0.090361  |  val_loss = 0.090016  |  training_for: 147.02
Traceback (most recent call last):
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 131, in <module>
    print('starting')
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 117, in main
    # unsupervised pretraining
  File "/home/jwilkie/code_base/packages/model_training/training_loops.py", line 150, in train
    loss = loss_calc(model, batch)  #compute validation loss
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/composite_loss.py", line 46, in calc_loss
    z_aug = model.feed_aug(x)
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/saint.py", line 141, in feed_aug
    return self.encoder(self.latent_aug(self.embedding_layer(self.feature_aug(x))))
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/encoders.py", line 55, in forward
    x = layer(x)
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/encoders.py", line 101, in forward
    x = self.sublayers[0](x, lambda z: self.self_attn(z, z, z)[0])
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/architecture.py", line 45, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/home/jwilkie/code_base/packages/custom_transformers/encoders.py", line 101, in <lambda>
    x = self.sublayers[0](x, lambda z: self.self_attn(z, z, z)[0])
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1130, in forward
    self.out_proj.bias,
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1601, in __getattr__
    def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
KeyboardInterrupt