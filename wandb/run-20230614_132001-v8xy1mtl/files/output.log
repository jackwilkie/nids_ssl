
epoch = 0  |  train_loss = 0.159849  |  val_loss = 0.155148  |  training_for: 1.82
epoch = 1  |  train_loss = 0.150964  |  val_loss = 0.152753  |  training_for: 3.34
epoch = 2  |  train_loss = 0.149586  |  val_loss = 0.152912  |  training_for: 4.82
epoch = 3  |  train_loss = 0.149022  |  val_loss = 0.152350  |  training_for: 6.30
epoch = 4  |  train_loss = 0.148662  |  val_loss = 0.153430  |  training_for: 7.78
epoch = 5  |  train_loss = 0.148456  |  val_loss = 0.154082  |  training_for: 9.26
epoch = 6  |  train_loss = 0.148319  |  val_loss = 0.156277  |  training_for: 10.73
epoch = 7  |  train_loss = 0.148219  |  val_loss = 0.155802  |  training_for: 12.21
epoch = 8  |  train_loss = 0.148081  |  val_loss = 0.156318  |  training_for: 13.69
epoch = 9  |  train_loss = 0.148019  |  val_loss = 0.157981  |  training_for: 15.19
epoch = 10  |  train_loss = 0.148042  |  val_loss = 0.157203  |  training_for: 16.77
epoch = 11  |  train_loss = 0.147939  |  val_loss = 0.157398  |  training_for: 18.24
epoch = 12  |  train_loss = 0.147942  |  val_loss = 0.157532  |  training_for: 19.72
epoch = 13  |  train_loss = 0.147913  |  val_loss = 0.157947  |  training_for: 21.20
epoch = 14  |  train_loss = 0.147884  |  val_loss = 0.158090  |  training_for: 22.69
epoch = 15  |  train_loss = 0.147842  |  val_loss = 0.159371  |  training_for: 24.17
epoch = 16  |  train_loss = 0.147847  |  val_loss = 0.158675  |  training_for: 25.67
epoch = 17  |  train_loss = 0.147830  |  val_loss = 0.157931  |  training_for: 27.14
epoch = 18  |  train_loss = 0.147827  |  val_loss = 0.159126  |  training_for: 28.69
epoch = 19  |  train_loss = 0.147808  |  val_loss = 0.159247  |  training_for: 30.24
epoch = 20  |  train_loss = 0.147787  |  val_loss = 0.158741  |  training_for: 31.77
epoch = 21  |  train_loss = 0.147873  |  val_loss = 0.159065  |  training_for: 33.29
epoch = 22  |  train_loss = 0.147789  |  val_loss = 0.158054  |  training_for: 34.79
epoch = 23  |  train_loss = 0.147735  |  val_loss = 0.158295  |  training_for: 36.46
epoch = 24  |  train_loss = 0.147699  |  val_loss = 0.158438  |  training_for: 38.02
epoch = 25  |  train_loss = 0.147695  |  val_loss = 0.157869  |  training_for: 39.59
epoch = 26  |  train_loss = 0.147694  |  val_loss = 0.157126  |  training_for: 41.15
epoch = 27  |  train_loss = 0.147737  |  val_loss = 0.158062  |  training_for: 42.69
epoch = 28  |  train_loss = 0.147712  |  val_loss = 0.159164  |  training_for: 44.30
epoch = 29  |  train_loss = 0.147703  |  val_loss = 0.157167  |  training_for: 45.79
epoch = 30  |  train_loss = 0.147694  |  val_loss = 0.158047  |  training_for: 47.34
epoch = 31  |  train_loss = 0.147699  |  val_loss = 0.158741  |  training_for: 48.89
epoch = 32  |  train_loss = 0.147736  |  val_loss = 0.157762  |  training_for: 50.38
epoch = 33  |  train_loss = 0.147731  |  val_loss = 0.158186  |  training_for: 51.85
epoch = 34  |  train_loss = 0.147692  |  val_loss = 0.158958  |  training_for: 53.33
epoch = 35  |  train_loss = 0.147707  |  val_loss = 0.158150  |  training_for: 54.82
epoch = 36  |  train_loss = 0.147707  |  val_loss = 0.158540  |  training_for: 56.40
epoch = 37  |  train_loss = 0.147676  |  val_loss = 0.159261  |  training_for: 57.88
epoch = 38  |  train_loss = 0.147733  |  val_loss = 0.159148  |  training_for: 59.36
epoch = 39  |  train_loss = 0.147705  |  val_loss = 0.159163  |  training_for: 60.85
epoch = 40  |  train_loss = 0.147742  |  val_loss = 0.158533  |  training_for: 62.34
epoch = 41  |  train_loss = 0.147713  |  val_loss = 0.158251  |  training_for: 63.81
epoch = 42  |  train_loss = 0.147691  |  val_loss = 0.158813  |  training_for: 65.28
epoch = 43  |  train_loss = 0.147645  |  val_loss = 0.158752  |  training_for: 66.78
epoch = 44  |  train_loss = 0.147643  |  val_loss = 0.158909  |  training_for: 68.34
epoch = 45  |  train_loss = 0.147670  |  val_loss = 0.158842  |  training_for: 69.90
epoch = 46  |  train_loss = 0.147670  |  val_loss = 0.159193  |  training_for: 71.44
epoch = 47  |  train_loss = 0.147675  |  val_loss = 0.159754  |  training_for: 72.92
epoch = 48  |  train_loss = 0.147669  |  val_loss = 0.158832  |  training_for: 74.42
epoch = 49  |  train_loss = 0.147619  |  val_loss = 0.158162  |  training_for: 75.91
epoch = 50  |  train_loss = 0.147612  |  val_loss = 0.158342  |  training_for: 77.50
epoch = 51  |  train_loss = 0.147653  |  val_loss = 0.158804  |  training_for: 78.99
epoch = 52  |  train_loss = 0.147620  |  val_loss = 0.159065  |  training_for: 80.48
epoch = 53  |  train_loss = 0.147620  |  val_loss = 0.158625  |  training_for: 81.97
epoch = 54  |  train_loss = 0.147640  |  val_loss = 0.158362  |  training_for: 83.45
epoch = 55  |  train_loss = 0.147690  |  val_loss = 0.159176  |  training_for: 84.94
epoch = 56  |  train_loss = 0.147616  |  val_loss = 0.158651  |  training_for: 86.43
epoch = 57  |  train_loss = 0.147616  |  val_loss = 0.157955  |  training_for: 87.91
epoch = 58  |  train_loss = 0.147621  |  val_loss = 0.159402  |  training_for: 89.40
epoch = 59  |  train_loss = 0.147634  |  val_loss = 0.158332  |  training_for: 90.90
epoch = 60  |  train_loss = 0.147655  |  val_loss = 0.159115  |  training_for: 92.40
epoch = 61  |  train_loss = 0.147628  |  val_loss = 0.158222  |  training_for: 93.89
epoch = 62  |  train_loss = 0.147654  |  val_loss = 0.159900  |  training_for: 95.38
epoch = 63  |  train_loss = 0.147662  |  val_loss = 0.160008  |  training_for: 96.97
epoch = 64  |  train_loss = 0.147653  |  val_loss = 0.159892  |  training_for: 98.46
epoch = 65  |  train_loss = 0.147616  |  val_loss = 0.160051  |  training_for: 99.96
epoch = 66  |  train_loss = 0.147603  |  val_loss = 0.159287  |  training_for: 101.48
epoch = 67  |  train_loss = 0.147588  |  val_loss = 0.158837  |  training_for: 102.97
epoch = 68  |  train_loss = 0.147584  |  val_loss = 0.160140  |  training_for: 104.49
epoch = 69  |  train_loss = 0.147594  |  val_loss = 0.159767  |  training_for: 106.01
epoch = 70  |  train_loss = 0.147564  |  val_loss = 0.160610  |  training_for: 107.49
epoch = 71  |  train_loss = 0.147545  |  val_loss = 0.159401  |  training_for: 108.97
epoch = 72  |  train_loss = 0.147560  |  val_loss = 0.158729  |  training_for: 110.47
epoch = 73  |  train_loss = 0.147593  |  val_loss = 0.160088  |  training_for: 111.95
epoch = 74  |  train_loss = 0.147613  |  val_loss = 0.160766  |  training_for: 113.44
epoch = 75  |  train_loss = 0.147610  |  val_loss = 0.159477  |  training_for: 114.94
epoch = 76  |  train_loss = 0.147577  |  val_loss = 0.159077  |  training_for: 116.53
epoch = 77  |  train_loss = 0.147605  |  val_loss = 0.160098  |  training_for: 118.03
epoch = 78  |  train_loss = 0.147611  |  val_loss = 0.159224  |  training_for: 119.52
epoch = 79  |  train_loss = 0.147625  |  val_loss = 0.161191  |  training_for: 121.05
epoch = 80  |  train_loss = 0.147626  |  val_loss = 0.160090  |  training_for: 122.53
epoch = 81  |  train_loss = 0.147605  |  val_loss = 0.160302  |  training_for: 124.04
epoch = 82  |  train_loss = 0.147585  |  val_loss = 0.159934  |  training_for: 125.53
epoch = 83  |  train_loss = 0.147583  |  val_loss = 0.160352  |  training_for: 127.01
epoch = 84  |  train_loss = 0.147606  |  val_loss = 0.160067  |  training_for: 128.54
epoch = 85  |  train_loss = 0.147604  |  val_loss = 0.161079  |  training_for: 130.04
epoch = 86  |  train_loss = 0.147603  |  val_loss = 0.159670  |  training_for: 131.52
epoch = 87  |  train_loss = 0.147543  |  val_loss = 0.160109  |  training_for: 133.00
epoch = 88  |  train_loss = 0.147528  |  val_loss = 0.159721  |  training_for: 134.48
epoch = 89  |  train_loss = 0.147551  |  val_loss = 0.160047  |  training_for: 135.99
epoch = 90  |  train_loss = 0.147557  |  val_loss = 0.159947  |  training_for: 137.61
epoch = 91  |  train_loss = 0.147560  |  val_loss = 0.160105  |  training_for: 139.11
epoch = 92  |  train_loss = 0.147546  |  val_loss = 0.159858  |  training_for: 140.63
epoch = 93  |  train_loss = 0.147574  |  val_loss = 0.160659  |  training_for: 142.12
epoch = 94  |  train_loss = 0.147566  |  val_loss = 0.160108  |  training_for: 143.60
epoch = 95  |  train_loss = 0.147545  |  val_loss = 0.159900  |  training_for: 145.09
epoch = 96  |  train_loss = 0.147539  |  val_loss = 0.159817  |  training_for: 146.58
epoch = 97  |  train_loss = 0.147564  |  val_loss = 0.159714  |  training_for: 148.07
epoch = 98  |  train_loss = 0.147566  |  val_loss = 0.159994  |  training_for: 149.55
epoch = 99  |  train_loss = 0.147554  |  val_loss = 0.158935  |  training_for: 151.06
epoch = 100  |  train_loss = 0.147602  |  val_loss = 0.160220  |  training_for: 152.55
epoch = 101  |  train_loss = 0.147580  |  val_loss = 0.160282  |  training_for: 154.03
epoch = 102  |  train_loss = 0.147590  |  val_loss = 0.160454  |  training_for: 155.51
epoch = 103  |  train_loss = 0.147567  |  val_loss = 0.159685  |  training_for: 157.10
epoch = 104  |  train_loss = 0.147537  |  val_loss = 0.158812  |  training_for: 158.59
epoch = 105  |  train_loss = 0.147554  |  val_loss = 0.161360  |  training_for: 160.07
epoch = 106  |  train_loss = 0.147550  |  val_loss = 0.159966  |  training_for: 161.56
epoch = 107  |  train_loss = 0.147579  |  val_loss = 0.160851  |  training_for: 163.08
epoch = 108  |  train_loss = 0.147589  |  val_loss = 0.159699  |  training_for: 164.56
epoch = 109  |  train_loss = 0.147585  |  val_loss = 0.159602  |  training_for: 166.11
epoch = 110  |  train_loss = 0.147583  |  val_loss = 0.159790  |  training_for: 167.60
epoch = 111  |  train_loss = 0.147547  |  val_loss = 0.159286  |  training_for: 169.07
epoch = 112  |  train_loss = 0.147557  |  val_loss = 0.161042  |  training_for: 170.56
epoch = 113  |  train_loss = 0.147573  |  val_loss = 0.158952  |  training_for: 172.05
epoch = 114  |  train_loss = 0.147523  |  val_loss = 0.159210  |  training_for: 173.54
epoch = 115  |  train_loss = 0.147543  |  val_loss = 0.160302  |  training_for: 175.04
epoch = 116  |  train_loss = 0.147541  |  val_loss = 0.158918  |  training_for: 176.52
epoch = 117  |  train_loss = 0.147539  |  val_loss = 0.159062  |  training_for: 178.10
epoch = 118  |  train_loss = 0.147524  |  val_loss = 0.159936  |  training_for: 179.58
epoch = 119  |  train_loss = 0.147515  |  val_loss = 0.159373  |  training_for: 181.09
epoch = 120  |  train_loss = 0.147550  |  val_loss = 0.159772  |  training_for: 182.56
epoch = 121  |  train_loss = 0.147561  |  val_loss = 0.159491  |  training_for: 184.05
epoch = 122  |  train_loss = 0.147567  |  val_loss = 0.160684  |  training_for: 185.55
epoch = 123  |  train_loss = 0.147541  |  val_loss = 0.159664  |  training_for: 187.05
epoch = 124  |  train_loss = 0.147545  |  val_loss = 0.159910  |  training_for: 188.53
epoch = 125  |  train_loss = 0.147526  |  val_loss = 0.159761  |  training_for: 190.00
epoch = 126  |  train_loss = 0.147496  |  val_loss = 0.159450  |  training_for: 191.48
epoch = 127  |  train_loss = 0.147489  |  val_loss = 0.159559  |  training_for: 192.96
epoch = 128  |  train_loss = 0.147485  |  val_loss = 0.158702  |  training_for: 194.46
epoch = 129  |  train_loss = 0.147513  |  val_loss = 0.159656  |  training_for: 195.96
epoch = 130  |  train_loss = 0.147526  |  val_loss = 0.158787  |  training_for: 197.53
epoch = 131  |  train_loss = 0.147504  |  val_loss = 0.160251  |  training_for: 199.01
epoch = 132  |  train_loss = 0.147527  |  val_loss = 0.158577  |  training_for: 200.51
epoch = 133  |  train_loss = 0.147522  |  val_loss = 0.158933  |  training_for: 201.99
epoch = 134  |  train_loss = 0.147539  |  val_loss = 0.159474  |  training_for: 203.48
epoch = 135  |  train_loss = 0.147540  |  val_loss = 0.159072  |  training_for: 204.97
epoch = 136  |  train_loss = 0.147534  |  val_loss = 0.159134  |  training_for: 206.46
epoch = 137  |  train_loss = 0.147516  |  val_loss = 0.158644  |  training_for: 207.95
epoch = 138  |  train_loss = 0.147499  |  val_loss = 0.160107  |  training_for: 209.45
epoch = 139  |  train_loss = 0.147517  |  val_loss = 0.158743  |  training_for: 210.94
epoch = 140  |  train_loss = 0.147492  |  val_loss = 0.159777  |  training_for: 212.43
epoch = 141  |  train_loss = 0.147489  |  val_loss = 0.158402  |  training_for: 213.92
epoch = 142  |  train_loss = 0.147494  |  val_loss = 0.159543  |  training_for: 215.40
epoch = 143  |  train_loss = 0.147512  |  val_loss = 0.159649  |  training_for: 216.89
epoch = 144  |  train_loss = 0.147581  |  val_loss = 0.158006  |  training_for: 218.48
epoch = 145  |  train_loss = 0.147552  |  val_loss = 0.158906  |  training_for: 219.97
epoch = 146  |  train_loss = 0.147519  |  val_loss = 0.158863  |  training_for: 221.47
epoch = 147  |  train_loss = 0.147525  |  val_loss = 0.159995  |  training_for: 222.96
epoch = 148  |  train_loss = 0.147544  |  val_loss = 0.159062  |  training_for: 224.45
epoch = 149  |  train_loss = 0.147524  |  val_loss = 0.159458  |  training_for: 225.94
epoch = 150  |  train_loss = 0.147503  |  val_loss = 0.159021  |  training_for: 227.43
epoch = 151  |  train_loss = 0.147490  |  val_loss = 0.158942  |  training_for: 228.93
epoch = 152  |  train_loss = 0.147493  |  val_loss = 0.158652  |  training_for: 230.42
epoch = 153  |  train_loss = 0.147493  |  val_loss = 0.159573  |  training_for: 231.89
epoch = 154  |  train_loss = 0.147510  |  val_loss = 0.158804  |  training_for: 233.38
epoch = 155  |  train_loss = 0.147491  |  val_loss = 0.159284  |  training_for: 234.92
epoch = 156  |  train_loss = 0.147490  |  val_loss = 0.160129  |  training_for: 236.40
epoch = 157  |  train_loss = 0.147505  |  val_loss = 0.159017  |  training_for: 238.00
epoch = 158  |  train_loss = 0.147507  |  val_loss = 0.159970  |  training_for: 239.49
epoch = 159  |  train_loss = 0.147506  |  val_loss = 0.159090  |  training_for: 240.99
epoch = 160  |  train_loss = 0.147501  |  val_loss = 0.158479  |  training_for: 242.49
epoch = 161  |  train_loss = 0.147520  |  val_loss = 0.159558  |  training_for: 243.98
epoch = 162  |  train_loss = 0.147538  |  val_loss = 0.158806  |  training_for: 245.48
epoch = 163  |  train_loss = 0.147518  |  val_loss = 0.159921  |  training_for: 246.95
epoch = 164  |  train_loss = 0.147502  |  val_loss = 0.159144  |  training_for: 248.45
epoch = 165  |  train_loss = 0.147480  |  val_loss = 0.157515  |  training_for: 249.93
epoch = 166  |  train_loss = 0.147520  |  val_loss = 0.159033  |  training_for: 251.42
epoch = 167  |  train_loss = 0.147518  |  val_loss = 0.159571  |  training_for: 252.91
epoch = 168  |  train_loss = 0.147516  |  val_loss = 0.158758  |  training_for: 254.40
epoch = 169  |  train_loss = 0.147492  |  val_loss = 0.158727  |  training_for: 255.88
epoch = 170  |  train_loss = 0.147496  |  val_loss = 0.158806  |  training_for: 257.37
epoch = 171  |  train_loss = 0.147462  |  val_loss = 0.158937  |  training_for: 258.96
epoch = 172  |  train_loss = 0.147467  |  val_loss = 0.158482  |  training_for: 260.45
epoch = 173  |  train_loss = 0.147488  |  val_loss = 0.158981  |  training_for: 261.95
epoch = 174  |  train_loss = 0.147485  |  val_loss = 0.158607  |  training_for: 263.44
epoch = 175  |  train_loss = 0.147471  |  val_loss = 0.158905  |  training_for: 264.92
epoch = 176  |  train_loss = 0.147501  |  val_loss = 0.158287  |  training_for: 266.41
epoch = 177  |  train_loss = 0.147507  |  val_loss = 0.159373  |  training_for: 267.90
epoch = 178  |  train_loss = 0.147514  |  val_loss = 0.158801  |  training_for: 269.39
epoch = 179  |  train_loss = 0.147508  |  val_loss = 0.159124  |  training_for: 270.88
epoch = 180  |  train_loss = 0.147486  |  val_loss = 0.158545  |  training_for: 272.37
epoch = 181  |  train_loss = 0.147491  |  val_loss = 0.158075  |  training_for: 273.89
epoch = 182  |  train_loss = 0.147480  |  val_loss = 0.158736  |  training_for: 275.38
epoch = 183  |  train_loss = 0.147497  |  val_loss = 0.158150  |  training_for: 276.87
epoch = 184  |  train_loss = 0.147490  |  val_loss = 0.158904  |  training_for: 278.36
epoch = 185  |  train_loss = 0.147473  |  val_loss = 0.159248  |  training_for: 279.94
epoch = 186  |  train_loss = 0.147462  |  val_loss = 0.159334  |  training_for: 281.42
epoch = 187  |  train_loss = 0.147497  |  val_loss = 0.159221  |  training_for: 282.91
epoch = 188  |  train_loss = 0.147496  |  val_loss = 0.158848  |  training_for: 284.40
epoch = 189  |  train_loss = 0.147514  |  val_loss = 0.159623  |  training_for: 285.89
epoch = 190  |  train_loss = 0.147523  |  val_loss = 0.158966  |  training_for: 287.37
epoch = 191  |  train_loss = 0.147525  |  val_loss = 0.158693  |  training_for: 288.86
epoch = 192  |  train_loss = 0.147504  |  val_loss = 0.159296  |  training_for: 290.36
epoch = 193  |  train_loss = 0.147491  |  val_loss = 0.159241  |  training_for: 291.85
epoch = 194  |  train_loss = 0.147476  |  val_loss = 0.159306  |  training_for: 293.34
epoch = 195  |  train_loss = 0.147472  |  val_loss = 0.158189  |  training_for: 294.84
epoch = 196  |  train_loss = 0.147486  |  val_loss = 0.158911  |  training_for: 296.33
epoch = 197  |  train_loss = 0.147480  |  val_loss = 0.158264  |  training_for: 297.86
epoch = 198  |  train_loss = 0.147499  |  val_loss = 0.158188  |  training_for: 299.34
epoch = 199  |  train_loss = 0.147480  |  val_loss = 0.158782  |  training_for: 300.93
epoch = 200  |  train_loss = 0.147479  |  val_loss = 0.158518  |  training_for: 302.45
epoch = 201  |  train_loss = 0.147506  |  val_loss = 0.159347  |  training_for: 303.95
epoch = 202  |  train_loss = 0.147503  |  val_loss = 0.159026  |  training_for: 305.45
epoch = 203  |  train_loss = 0.147506  |  val_loss = 0.159117  |  training_for: 306.95
epoch = 204  |  train_loss = 0.147491  |  val_loss = 0.158497  |  training_for: 308.44
epoch = 205  |  train_loss = 0.147472  |  val_loss = 0.158978  |  training_for: 309.93
epoch = 206  |  train_loss = 0.147475  |  val_loss = 0.157844  |  training_for: 311.42
epoch = 207  |  train_loss = 0.147489  |  val_loss = 0.158291  |  training_for: 312.90
epoch = 208  |  train_loss = 0.147472  |  val_loss = 0.158408  |  training_for: 314.37
epoch = 209  |  train_loss = 0.147460  |  val_loss = 0.158606  |  training_for: 315.86
epoch = 210  |  train_loss = 0.147458  |  val_loss = 0.157889  |  training_for: 317.36
epoch = 211  |  train_loss = 0.147441  |  val_loss = 0.158177  |  training_for: 318.84
epoch = 212  |  train_loss = 0.147435  |  val_loss = 0.158200  |  training_for: 320.33
epoch = 213  |  train_loss = 0.147455  |  val_loss = 0.158793  |  training_for: 321.92
epoch = 214  |  train_loss = 0.147432  |  val_loss = 0.157730  |  training_for: 323.41
epoch = 215  |  train_loss = 0.147439  |  val_loss = 0.158100  |  training_for: 324.90
epoch = 216  |  train_loss = 0.147446  |  val_loss = 0.157704  |  training_for: 326.41
epoch = 217  |  train_loss = 0.147464  |  val_loss = 0.157945  |  training_for: 327.91
epoch = 218  |  train_loss = 0.147474  |  val_loss = 0.158813  |  training_for: 329.40
epoch = 219  |  train_loss = 0.147472  |  val_loss = 0.157639  |  training_for: 330.88
epoch = 220  |  train_loss = 0.147489  |  val_loss = 0.158040  |  training_for: 332.37
epoch = 221  |  train_loss = 0.147489  |  val_loss = 0.157837  |  training_for: 333.87
epoch = 222  |  train_loss = 0.147471  |  val_loss = 0.157882  |  training_for: 335.36
epoch = 223  |  train_loss = 0.147455  |  val_loss = 0.157760  |  training_for: 336.92
epoch = 224  |  train_loss = 0.147455  |  val_loss = 0.157939  |  training_for: 338.41
epoch = 225  |  train_loss = 0.147452  |  val_loss = 0.157539  |  training_for: 339.92
epoch = 226  |  train_loss = 0.147482  |  val_loss = 0.157707  |  training_for: 341.50
epoch = 227  |  train_loss = 0.147477  |  val_loss = 0.158079  |  training_for: 342.98
epoch = 228  |  train_loss = 0.147472  |  val_loss = 0.157787  |  training_for: 344.47
epoch = 229  |  train_loss = 0.147455  |  val_loss = 0.158279  |  training_for: 345.97
epoch = 230  |  train_loss = 0.147446  |  val_loss = 0.158457  |  training_for: 347.46
epoch = 231  |  train_loss = 0.147441  |  val_loss = 0.157656  |  training_for: 348.99
epoch = 232  |  train_loss = 0.147456  |  val_loss = 0.158591  |  training_for: 350.48
epoch = 233  |  train_loss = 0.147471  |  val_loss = 0.157574  |  training_for: 351.96
epoch = 234  |  train_loss = 0.147457  |  val_loss = 0.157797  |  training_for: 353.45
epoch = 235  |  train_loss = 0.147479  |  val_loss = 0.158328  |  training_for: 354.97
epoch = 236  |  train_loss = 0.147469  |  val_loss = 0.157440  |  training_for: 356.46
epoch = 237  |  train_loss = 0.147447  |  val_loss = 0.157760  |  training_for: 357.96
epoch = 238  |  train_loss = 0.147464  |  val_loss = 0.157784  |  training_for: 359.46
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/jwilkie/code_base/nids_ssl/main.py", line 114, in main
    train(model = model,
  File "/home/jwilkie/code_base/packages/model_training/training_loops.py", line 118, in train
    loss = loss_calc(model, batch)  #compute training loss
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/composite_loss.py", line 46, in calc_loss
    z_aug = model.feed_aug(x)
  File "/home/jwilkie/code_base/packages/self_supervised/tabular/saint/saint.py", line 141, in feed_aug
    return self.encoder(self.latent_aug(self.embedding_layer(self.feature_aug(x))))
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/encoders.py", line 55, in forward
    x = layer(x)
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/encoders.py", line 104, in forward
    return self.sublayers[1](x, self.feed_forward)
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/architecture.py", line 45, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/code_base/packages/custom_transformers/architecture.py", line 135, in forward
    return self.w2(self.dropout(self.activation(self.w1(x))))  # apply forward pass
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jwilkie/pylabs/env/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.69 GiB total capacity; 21.92 GiB already allocated; 15.81 MiB free; 23.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.